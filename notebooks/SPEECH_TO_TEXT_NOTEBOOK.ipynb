{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "uyvg4lyxferanka4nrmq",
   "authorId": "61864603178",
   "authorName": "ADMIN",
   "authorEmail": "michael.gorkow@snowflake.com",
   "sessionId": "a643ec2c-0519-4477-9726-4a59225d88c2",
   "lastEditTime": 1744595005066
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e8d535-b9bb-4c41-8b82-d695fdddabfd",
   "metadata": {
    "name": "TITLE",
    "collapsed": false
   },
   "source": "# Speech to Text in Snowflake\nThis notebook walks you through the following steps:\n* Define a custom model for multiple whisper models.\n* Register the model in Snowflake's model registry\n* Deploy the model as an inference service using Snowpark Container Services\n* Test the deployed inference service\n* View Service Logs"
  },
  {
   "cell_type": "markdown",
   "id": "b2fc207f-360c-4ca1-b3b2-81ee724f0609",
   "metadata": {
    "name": "INSTALL_PACKAGES1",
    "collapsed": false
   },
   "source": "## Install additional libraries"
  },
  {
   "cell_type": "code",
   "id": "f8f5ff30-d173-44f8-9b23-f7158cd4845b",
   "metadata": {
    "language": "python",
    "name": "INSTALL_PACKAGES2"
   },
   "outputs": [],
   "source": "!pip install soundfile --quiet",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ecc1e9f8-5b80-49bf-a422-34ab599be30a",
   "metadata": {
    "name": "CONNECTION1",
    "collapsed": false
   },
   "source": "## Create Connection"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "CONNECTION2"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\", FutureWarning)\nwarnings.simplefilter(\"ignore\", UserWarning)\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "61a9e73d-eae6-4a6d-8ed8-7fb5a0d53a3e",
   "metadata": {
    "name": "DEFINE_MODEL1",
    "collapsed": false
   },
   "source": "## Define Model\nWe'll be hosting multiple whisper models on the same GPU."
  },
  {
   "cell_type": "code",
   "id": "6152ff24-8bd1-463a-b1c8-bfd88ed7807a",
   "metadata": {
    "language": "python",
    "name": "DEFINE_MODEL2"
   },
   "outputs": [],
   "source": "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nimport torch\n\ndef load_pipeline(model_id):\n    device = 0 if torch.cuda.is_available() else \"cpu\"\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n        model_id,\n        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=True,\n        use_safetensors=True\n    ).to(device)\n    \n    processor = AutoProcessor.from_pretrained(model_id)\n\n    model_pipeline = pipeline(\n        \"automatic-speech-recognition\",\n        model=model,\n        tokenizer=processor.tokenizer,\n        feature_extractor=processor.feature_extractor,\n        torch_dtype=torch_dtype,\n        device=device,\n    )\n    \n    return model_pipeline\n    \n#model_pipeline = load_pipeline('openai/whisper-large-v3-turbo')\n\npipelines = {}\nfor code in ['tiny','base','small','medium','large-v3-turbo']:\n    pipelines[code] = load_pipeline(f'openai/whisper-{code}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "deb10190-e308-4536-abec-bb02aef1a679",
   "metadata": {
    "name": "CUSTOM_MODEL1",
    "collapsed": false
   },
   "source": "## Create Custom Model"
  },
  {
   "cell_type": "code",
   "id": "94b37b91-6d7c-4059-852e-4ac9a52ed116",
   "metadata": {
    "language": "python",
    "name": "CUSTOM_MODEL2"
   },
   "outputs": [],
   "source": "from snowflake.ml.model import custom_model\nimport soundfile as sf\nfrom scipy.signal import resample\nimport numpy as np\nimport io\nimport logging\n\nclass SpeechToTextModel(custom_model.CustomModel):\n    def __init__(self, context: custom_model.ModelContext) -> None:\n        super().__init__(context)\n        warnings.simplefilter(\"ignore\", FutureWarning)\n\n        # Set up a logger\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.logger.handlers.clear()\n\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n\n    def run_pipeline(self, audio, model):\n        self.logger.debug(f\"Starting speech to text processing ...\")\n        # Test if hex-encoded bytes or real bytes\n        try:\n            audio = bytes.fromhex(audio.decode(\"ascii\"))\n        except:\n            pass\n        \n        # Load with soundfile\n        audio_data, sample_rate = sf.read(io.BytesIO(audio))\n\n        # Convert stereo to mono if needed\n        if len(audio_data.shape) > 1:\n            audio_data = np.mean(audio_data, axis=1)\n\n        # Resample to 16000 Hz if needed\n        target_rate = 16000\n        if sample_rate != target_rate:\n            num_samples = int(len(audio_data) * target_rate / sample_rate)\n            audio_data = resample(audio_data, num_samples)\n            sample_rate = target_rate\n            \n        text = self.context[model]({\"array\": audio_data.astype(np.float32), \"sampling_rate\": sample_rate})['text']\n        self.logger.debug(f\"Finished speech to text processing ...\")\n        return text\n\n    @custom_model.inference_api\n    def transform(self, audio_df: pd.DataFrame) -> pd.DataFrame:\n        transcriptions = audio_df.apply(lambda x: self.run_pipeline(x['AUDIO_INPUT'],x['MODEL']), axis=1)\n        result = pd.DataFrame({'TRANSCRIPTION':transcriptions})\n        return result\n\n# Set the model context that includes the model pipeline\nmc = custom_model.ModelContext(\n    tiny = pipelines['tiny'], \n    base = pipelines['base'], \n    small = pipelines['small'],\n    medium = pipelines['medium'],\n    large_v3_turbo = pipelines['large-v3-turbo'],\n)\n\n# Set the model context that includes the model pipeline\n#mc = custom_model.ModelContext(model_pipeline=model_pipeline)\nspeech_to_text_model = SpeechToTextModel(context=mc)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d769ad9-b27e-4dc7-b9e8-b69b949d0f48",
   "metadata": {
    "name": "TEST_MODEL1",
    "collapsed": false
   },
   "source": "## Test Model"
  },
  {
   "cell_type": "code",
   "id": "9187caeb-bdfc-46eb-a3b8-3d6081108a01",
   "metadata": {
    "language": "python",
    "name": "TEST_MODEL2"
   },
   "outputs": [],
   "source": "with open('harvard.wav', 'rb') as f:\n    audio_input = f.read()\n\ninput_df = pd.DataFrame([[audio_input,'small']], columns=['AUDIO_INPUT','MODEL'])\noutput_df = speech_to_text_model.transform(input_df)\n\n# Read results\nfor ix, row in pd.concat([input_df, output_df], axis=1).iterrows():\n    with st.chat_message('ai'):\n        st.write(row['TRANSCRIPTION'])\n        st.audio(row['AUDIO_INPUT'])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb528297-acf9-4344-83e5-71d67ba27107",
   "metadata": {
    "name": "REGISTER_MODEL1",
    "collapsed": false
   },
   "source": "## Register Model"
  },
  {
   "cell_type": "code",
   "id": "8fbf2689-7b20-4f2b-b92e-857a7593cdf9",
   "metadata": {
    "language": "sql",
    "name": "REGISTER_MODEL2"
   },
   "outputs": [],
   "source": "CREATE SCHEMA IF NOT EXISTS MODEL_REGISTRY;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d13ff358-ca62-44ed-86a9-6336de98a1b8",
   "metadata": {
    "language": "python",
    "name": "REGISTER_MODEL3"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\nfrom snowflake.ml.model.model_signature import infer_signature\n\nreg = Registry(session=session, database_name=\"AUDIO_INTERFACING_DEMO\", schema_name=\"MODEL_REGISTRY\")\n\nmodel_signature = infer_signature(input_data=input_df, output_data=output_df)\nprint(model_signature)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bffe33f-eda7-49e1-9521-2072032e70ce",
   "metadata": {
    "language": "python",
    "name": "REGISTER_MODEL4"
   },
   "outputs": [],
   "source": "model_ref = reg.log_model(\n    model_name=\"SPEECH_TO_TEXT\",\n    version_name=\"MULTIPLE\",    \n    model=speech_to_text_model,\n    pip_requirements=['torch','soundfile'],\n    signatures={\"transform\": model_signature},\n    options={\"use_gpu\": True, \"cuda_version\": \"11.8\"},\n    comment=\"openai/whisper [tiny, base, small, medium, large-v3-turbo]\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7bce56a-d7e3-4afe-8a7f-f2382d4e97f9",
   "metadata": {
    "name": "CREATE_SERVICE1",
    "collapsed": false
   },
   "source": "## Create Inference Service"
  },
  {
   "cell_type": "code",
   "id": "0e40fa4a-aa72-4fea-ad27-2fd304fa247e",
   "metadata": {
    "language": "python",
    "name": "CREATE_SERVICE2"
   },
   "outputs": [],
   "source": "inference_service = model_ref.create_service(\n    service_name=\"AUDIO_INTERFACING_DEMO.PUBLIC.SPEECH_TO_TEXT\",\n    service_compute_pool=\"AUDIO_INTERFACE_GPU_POOL\",\n    image_repo=\"AUDIO_INTERFACING_DEMO.PUBLIC.IMAGE_REPO_SERVICES\",\n    ingress_enabled=True,\n    gpu_requests='1',\n    build_external_access_integration=\"hf_pypi_access_integration\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb3e7a24-574e-4f7f-86de-41937aab4a2f",
   "metadata": {
    "language": "python",
    "name": "CREATE_SERVICE3"
   },
   "outputs": [],
   "source": "model_ref.list_services()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7acb7d7f-afc4-4fb1-b4ac-637e621180c1",
   "metadata": {
    "name": "TEST_SERVICE1",
    "collapsed": false
   },
   "source": "## Test Inference Service"
  },
  {
   "cell_type": "code",
   "id": "e593460c-665c-403d-9432-dc0088f0b7bb",
   "metadata": {
    "language": "python",
    "name": "TEST_SERVICE2"
   },
   "outputs": [],
   "source": "output_df = model_ref.run(\n    input_df,\n    function_name=\"transform\",\n    service_name=\"AUDIO_INTERFACING_DEMO.PUBLIC.SPEECH_TO_TEXT\"\n)\n\n# Read results\nfor ix, row in pd.concat([input_df, output_df], axis=1).iterrows():\n    with st.chat_message('ai'):\n        st.write(row['TRANSCRIPTION'])\n        st.audio(row['AUDIO_INPUT'])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73289799-a076-42a1-8f04-b5b14f880039",
   "metadata": {
    "name": "LOGS1",
    "collapsed": false
   },
   "source": "## View Logs"
  },
  {
   "cell_type": "code",
   "id": "cbc0ccd6-dd22-47cf-897f-bb6e5fa60dbf",
   "metadata": {
    "language": "python",
    "name": "LOGS2"
   },
   "outputs": [],
   "source": "logs = session.call('SYSTEM$GET_SERVICE_LOGS', 'AUDIO_INTERFACING_DEMO.PUBLIC.SPEECH_TO_TEXT', '0', 'model-inference')\nfor line in logs.split('\\n'):\n    print(line)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bef30c99-bff9-41d2-b327-273338a4325f",
   "metadata": {
    "name": "END",
    "collapsed": false
   },
   "source": "## END"
  }
 ]
}